Requirements:
write a FASTAPI (python) project that conducts the training of model using data parallelism, where the master dataaset is distributed among multiple workers, each worker has to submit its weights to parameter server so that each worker can fetch the latest weights from parameter server, or you can implement in your own way but the goal is to sync the training process across the workers during the training process asynchronously, and try to share the weights as soon as possible, like not at the end of the epoch infact i would suggest we should share after every example (image or 2 or 3 images), parameter server has to make sure that the weights are aggrigated before letting workers to fetch the latest weights, but i want the whole process to be very efficient and very fast, the parameter server should get the training parameters as input in json format including the master dataset path (http path) where from it can evenly distribute the example images among the workers, and only share the list of dataset files paths to workers so that workers can download their examples images and conduct the training, the model name, algo name and other parameters are also sent to the workers that are received by parameter server, let me share the example JSON that is received by parameter server , there is anothe project i.e. build in django (named: deep-distribute) , it receives the user request from client browser and it asks the parameter server to start training, now we have to desing this ps / worker setup to perform the training, the django endpoint is: def start_training_post(request): if request.method == 'POST': # Extract data from the POST request dataset_id = request.POST.get('dataset_id') model = request.POST.get('model') strategy = request.POST.get('strategy') #cluster = request.POST.get('cluster') cluster = 3 epochs = request.POST.get('epochs', 10) # Default to 10 if not provided batch_size = request.POST.get('batch_size', 32) # Default to 32 if not provided learning_rate = request.POST.get('learning_rate', 0.001) # Default to 0.001 if not provided optimizer = request.POST.get('optimizer', 'adam') # Default to 'adam' if not provided loss_function = request.POST.get('loss_function', 'binary_crossentropy') # Default to binary_crossentropy validation_split = request.POST.get('validation_split', 0.2) # Default to 0.2 if not provided early_stopping_patience = request.POST.get('early_stopping_patience', 10) # Default to 10 if not provided dropout_rate = request.POST.get('dropout_rate', 0.5) augmentation = request.POST.get('augmentation', 'None') # Default to 'None' if not provided class_weights = request.POST.get('class_weights', '{0:1.0, 1:1.0}') # Default to '{0:1.0, 1:1.0}' if not provided random_seed = request.POST.get('random_seed', '42') # Default to '42' if not provided training_params = { 'dataset_id': dataset_id, 'user': request.user, 'strategy': strategy, 'cluster': cluster, 'algo_name': model, 'epochs': epochs, 'batch_size' : batch_size, 'learning_rate' : learning_rate, 'optimizer' : optimizer, 'loss_function': loss_function, 'validation_split': validation_split, 'early_stopping_patience': early_stopping_patience, 'dropout_rate': dropout_rate, 'augmentation': augmentation, 'class_weights': class_weights, 'random_seed': random_seed } #delete other training objects by dataset to keep only one job per dataset TrainingJobService.delete_by_dataset_img_id(dataset_id) TrainingJobService.startTraining(training_params) messages.success(request, "Training Job Started successfully.") else: messages.error(request, "Cannot strat training.") return util.redirect('start_training' , dataset_id) def startTraining(training_params): algo_name=training_params['algo_name'] user=training_params['user'] dataset_id=training_params['dataset_id'] strategy=training_params['strategy'] training_params['user'] = user.id # save job training_job = TrainingJobService.create(dataset_id=dataset_id, user=user, algo_name=algo_name, parameter_settings=training_params) training_params['user'] = user training_params['training_job'] = training_job print (training_job.id) # get job_id and start training #trainingJob = TrainingJobService.get(job_id) #TrainingService.start_training_process(trainingJob.id) lif strategy == 3 or strategy=='GPU Cluster Custom': TrainingServiceCustom.start_training(training_params) else: # Handle any other cases if needed pass # or do something else return training_job.id import requests from django.conf import settings from django.forms.models import model_to_dict import json from train.dao.DatasetImgDAO import DatasetImgDAO from train.dao.ClusterNodeDAO import ClusterNodeDAO class TrainingServiceCustom: @staticmethod def start_training(training_params): # URL of the FastAPI endpoint to post the training params #url = settings.CUSTOM_TRAINING_PS_URL ps_url = "" cluster_id = training_params["cluster"] nodes = ClusterNodeDAO.get_by_cluster_node_type(cluster_id,'ps') if nodes: ps_url = f"http://{nodes[0].ip_address}:{nodes[0].port}/training-jobs/start" else: return None training_job = model_to_dict(training_params.get("training_job")) parameter_settings = training_job["parameter_settings"] try: del parameter_settings["training_job"] del parameter_settings["user"] except: print ("unable to delete training_job key: del parameter_settings['training_job']") if isinstance(parameter_settings, str): parameter_settings= json.loads(parameter_settings) job_data = { "job_id": training_job['id'], "job_name": training_job['job_name'], "dataset_img": training_job["dataset_img"], "status": training_job["status"], "algo": training_job["algo"], "user": training_job["user"], "parameter_settings": parameter_settings, } init_params ={} init_params["job_data"] = job_data init_params = set_dataset_details(training_job["dataset_img"], init_params) payload = { "init_params": init_params } try: ps_url +="/"+str(training_job['id']) # Sending POST request to FastAPI response = requests.post(ps_url, json=payload) # Check if the request was successful (status code 200) if response.status_code == 200: return response.json() else: raise Exception(f"Error in training request: {response.text}") except Exception as e: print(f"Error during HTTP request to FastAPI: {str(e)}") return None def set_dataset_details(dataset_id, init_params): dataset = DatasetImgDAO.get(dataset_id) init_params["dataset_details"] = dataset.gather_dataset_stats(include_images_url=True) init_params["dataset_details"]["host_url"] = settings.DATASET_HOST_URL return init_params this file can be used as is in the new project in workers to create the model by its name provided in json params: import tensorflow as tf DenseNet121 = tf.keras.applications.DenseNet121 DenseNet169 = tf.keras.applications.DenseNet169 DenseNet201 = tf.keras.applications.DenseNet201 EfficientNetB0 = tf.keras.applications.EfficientNetB0 EfficientNetB1 = tf.keras.applications.EfficientNetB1 EfficientNetB2 = tf.keras.applications.EfficientNetB2 EfficientNetB3 = tf.keras.applications.EfficientNetB3 EfficientNetB4 = tf.keras.applications.EfficientNetB4 EfficientNetB5 = tf.keras.applications.EfficientNetB5 EfficientNetB6 = tf.keras.applications.EfficientNetB6 EfficientNetB7 = tf.keras.applications.EfficientNetB7 InceptionV3 = tf.keras.applications.InceptionV3 MobileNet = tf.keras.applications.MobileNet MobileNetV2 = tf.keras.applications.MobileNetV2 NASNetLarge = tf.keras.applications.NASNetLarge NASNetMobile = tf.keras.applications.NASNetMobile ResNet50 = tf.keras.applications.ResNet50 ResNet50V2 = tf.keras.applications.ResNet50V2 ResNet101 = tf.keras.applications.ResNet101 ResNet101V2 = tf.keras.applications.ResNet101V2 ResNet152 = tf.keras.applications.ResNet152 ResNet152V2 = tf.keras.applications.ResNet152V2 VGG16 = tf.keras.applications.VGG16 VGG19 = tf.keras.applications.VGG19 Xception = tf.keras.applications.Xception class KerasCatalogService: MODELS_DICT = { "DenseNet121": DenseNet121, "DenseNet169": DenseNet169, "DenseNet201": DenseNet201, "EfficientNetB0": EfficientNetB0, "EfficientNetB1": EfficientNetB1, "EfficientNetB2": EfficientNetB2, "EfficientNetB3": EfficientNetB3, "EfficientNetB4": EfficientNetB4, "EfficientNetB5": EfficientNetB5, "EfficientNetB6": EfficientNetB6, "EfficientNetB7": EfficientNetB7, "InceptionV3": InceptionV3, "MobileNet": MobileNet, "MobileNetV2": MobileNetV2, "NASNetLarge": NASNetLarge, "NASNetMobile": NASNetMobile, "ResNet50": ResNet50, "ResNet50V2": ResNet50V2, "ResNet101": ResNet101, "ResNet101V2": ResNet101V2, "ResNet152": ResNet152, "ResNet152V2": ResNet152V2, "VGG16": VGG16, "VGG19": VGG19, "Xception": Xception } @staticmethod def list_all_models(): """Returns a list of all available Keras models.""" return KerasCatalogService.MODELS_DICT @staticmethod def list_all_strategies(): """Returns a list of all available Keras models.""" return [ "Single GPU", "GPU Cluster Parameter Server", "GPU Cluster Custom", ] @staticmethod def list_all_layers(): """Returns a list of all available Keras layers.""" layers_module = tf.keras.layers return [cls for cls in dir(layers_module) if isinstance(getattr(layers_module, cls), type)] @staticmethod def list_all_optimizers(): """Returns a list of all available Keras optimizers.""" optimizers_module = tf.keras.optimizers return [cls for cls in dir(optimizers_module) if isinstance(getattr(optimizers_module, cls), type)] @staticmethod def get_model_object(algo_name): input_shape = (150, 150, 3) # You can adjust this based on your requirement if algo_name not in KerasCatalogService.MODELS_DICT: raise ValueError(f"Algorithm '{algo_name}' is not available. Choose from {list(KerasCatalogService.MODELS_DICT)}") model_constructor = KerasCatalogService.MODELS_DICT[algo_name] base_model = model_constructor(weights='imagenet', include_top=False, input_shape=input_shape) return base_model def init_all_models(): for algo_name in KerasCatalogService.MODELS_DICT: print(algo_name) if algo_name =="NASNetLarge": input_shape=(331, 331, 3) else: input_shape=(224, 224, 3) model_constructor = KerasCatalogService.MODELS_DICT[algo_name] base_model = model_constructor(weights='imagenet', include_top=False, input_shape=input_shape) print("="*100) if __name__=='__main__': init_all_models() you need to create the project plan first the file structures dont write the code i will review the plan and then we will together create the files one by one,


===

you might have the knowledge of tensorflow, i need to beat this dataparallelism strategy, by updating the workers asynchronously very frequeantly to improve the accuracy and precision but not compromising the efficiency . so, make sure the weights sharing among the workers should be efficient, here is the strategy that i want to beat: def start_training(training_params): TrainingServicePS.init_tf_config() from train.dao.TrainingJobDAO import TrainingJobDAO print('Start training called...') job = training_params['training_job'] # Define the cluster specification # Set up the cluster resolver and strategy tf_config = os.environ.get("TF_CONFIG") print("****************************") print(tf_config) print("****************************") if not tf_config: raise ValueError("TF_CONFIG environment variable is not set!") #cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver() cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver() strategy = tf.distribute.experimental.ParameterServerStrategy(cluster_resolver) # Setup the coordinator coordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(strategy) # Loss function with Reduction.NONE loss_object = BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE) # Define global_batch_size appropriately global_batch_size = 20 # Adjust based on your setup def load_base_dataset(): data_dir = job.dataset_img.extracted_path raw_dataset = tf.keras.preprocessing.image_dataset_from_directory( data_dir, image_size=(150, 150), batch_size=global_batch_size, label_mode='categorical' # changed from 'binary' to support multi-class ) class_names = raw_dataset.class_names return raw_dataset.prefetch(tf.data.experimental.AUTOTUNE), len(class_names) def train_step_fn(images, labels): with tf.GradientTape() as tape: predictions = model(images, training=True) per_example_loss = loss_object(labels, predictions) loss = tf.reduce_sum(per_example_loss) * (1. / global_batch_size) grads = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(grads, model.trainable_variables)) return loss @tf.function def per_worker_train_step(iterator): images, labels = next(iterator) return strategy.run(train_step_fn, args=(images, labels)) def dataset_fn(): data_dir = job.dataset_img.extracted_path batch_size = global_batch_size img_height = 150 img_width = 150 dataset = tf.keras.preprocessing.image_dataset_from_directory( data_dir, image_size=(img_height, img_width), batch_size=batch_size, label_mode='binary' ).prefetch(tf.data.experimental.AUTOTUNE) print("Dataset loaded successfully.", flush=True) return dataset dataset_prefetched, num_classes = load_base_dataset() # Create the model under the strategy scope with strategy.scope(): '''model = Sequential([ Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)), MaxPooling2D(2, 2), Conv2D(64, (3, 3), activation='relu'), MaxPooling2D(2, 2), Flatten(), Dense(128, activation='relu'), Dense(1, activation='sigmoid') ]) optimizer = Adam() model.compile(optimizer=optimizer, loss=loss_object, metrics=['accuracy', 'Precision', 'Recall', 'AUC']) ''' #base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(150, 150, 3)) # Dynamically load the model based on model_name base_model = KerasCatalogService.get_model_object(training_params['algo_name']) for layer in base_model.layers: layer.trainable = False x = base_model.output x = GlobalAveragePooling2D()(x) x = Dense(1024, activation='relu')(x) predictions = Dense(1, activation='sigmoid')(x) model = Model(inputs=base_model.input, outputs=predictions) optimizer = Adam() model.compile(optimizer=optimizer, loss=loss_object, metrics=['accuracy', 'Precision', 'Recall', 'AUC']) # Create and distribute the dataset distributed_dataset = coordinator.create_per_worker_dataset(dataset_fn) distributed_iterator = iter(distributed_dataset) # Training loop for epoch in range(10): # Number of epochs start_epoch = time.time() print(f"Starting epoch {epoch + 1}") batch_index = 0 start_batch = time.time() try: coordinator.schedule(per_worker_train_step, args=(distributed_iterator,)) print(f"Batch {batch_index} processed in {time.time() - start_batch} seconds.") batch_index += 1 except tf.errors.OutOfRangeError: print("There are no more batches to process") coordinator.join() # Wait for all tasks to complete print(f"Epoch {epoch + 1} completed in {time.time() - start_epoch} seconds.") distributed_iterator = iter(distributed_dataset) # Reset iterator for the next epoch def eval_dataset_fn(): data_dir = job.dataset_img.extracted_path batch_size = global_batch_size img_height = 150 img_width = 150 dataset = tf.keras.preprocessing.image_dataset_from_directory( data_dir, image_size=(img_height, img_width), batch_size=batch_size, label_mode='binary' ).prefetch(tf.data.experimental.AUTOTUNE) return dataset # Create and distribute the evaluation dataset eval_distributed_dataset = coordinator.create_per_worker_dataset(eval_dataset_fn) eval_distributed_iterator = iter(eval_distributed_dataset) @tf.function def per_worker_eval_step(iterator): def step_fn(inputs): images, labels = inputs predictions = model(images, training=False) accuracy_metric.update_state(labels, predictions) precision_metric.update_state(labels, predictions) recall_metric.update_state(labels, predictions) auc_metric.update_state(labels, predictions) return strategy.run(step_fn, args=(next(iterator),)) # Evaluation loop accuracy_metric = tf.keras.metrics.BinaryAccuracy() precision_metric = tf.keras.metrics.Precision() recall_metric = tf.keras.metrics.Recall() auc_metric = tf.keras.metrics.AUC() while True: try: coordinator.schedule(per_worker_eval_step, args=(eval_distributed_iterator,)) except tf.errors.OutOfRangeError: break coordinator.join() final_accuracy = accuracy_metric.result().numpy() final_precision = precision_metric.result().numpy() final_recall = recall_metric.result().numpy() final_auc = auc_metric.result().numpy() final_f1 = 2 * (final_precision * final_recall) / (final_precision + final_recall + 1e-7) ended_at = timezone.now() # Update job status and accuracy upon completion results = { 'accuracy': float(final_accuracy), 'precision': float(final_precision), 'recall': float(final_recall), 'auc': float(final_auc), 'f1_score': float(final_f1) } results_json = json.dumps(results) TrainingJobDAO.update(job.id, status=JobStatus.COMPLETED.value, result=results_json, ended_at=ended_at) print(f'Training complete. Final accuracy: {final_accuracy}') every worker has the parameter server url and it register itself to the parameter server on starup. make sure the parameter server and workers are kept in one project , and when i setup this on the LAN on multiple machines i can change the flag wether it is a worker or a ps, secondly also try to connect to the postgresql database, we will enhance the code and connect to centralized database that is used by the deep-distribute (Django) project, because django creates a training job in database and we may update the database during the training process so that the django project can display this on front end browser . here is the database credentials: 'ENGINE': 'django.db.backends.postgresql_psycopg2', 'NAME': 'deep-distribute', 'USER': 'postgres', #'PASSWORD': 'szabist', 'PASSWORD': 'window', 'HOST': 'localhost', 'PORT': '5432', 'CONN_MAX_AGE': 1 also create .env file and read all these configurations from .env file here is the public.train_training_job definition CREATE TABLE public.train_training_job ( id serial4 NOT NULL, job_name varchar(300) NOT NULL, status varchar(300) NOT NULL, started_at timestamptz NULL, ended_at timestamptz NULL, algo varchar(300) NOT NULL, dataset_img_id int4 NOT NULL, user_id int4 NOT NULL, "result" text NULL, parameter_settings text NULL, training_log text NULL, training_log_history text NULL, CONSTRAINT train_training_job_pkey PRIMARY KEY (id), CONSTRAINT train_training_job_dataset_img_id_099728da_fk_train_dat FOREIGN KEY (dataset_img_id) REFERENCES public.train_dataset_img(id) DEFERRABLE INITIALLY DEFERRED, CONSTRAINT train_training_job_user_id_1526950c_fk_auth_user_id FOREIGN KEY (user_id) REFERENCES public.auth_user(id) DEFERRABLE INITIALLY DEFERRED ); CREATE INDEX train_training_job_dataset_img_id_099728da ON public.train_training_job USING btree (dataset_img_id); CREATE INDEX train_training_job_user_id_1526950c ON public.train_training_job USING btree (user_id);


ok, i am almost done with plan for now, lets start writing the quality, error free and efficient code, make sure we don't over look any requriement, it should be structured and efficient code, if it is difficult for you to create all the files at onces, just create one file at a time but make sure don't miss any thing , stick to the plan